{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset\n",
    "Obtained from https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_table('SMSSpamCollection', header = None, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(dataset.info())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class distribution\n",
    "print(dataset[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n",
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "5    spam\n",
      "6     ham\n",
      "7     ham\n",
      "8    spam\n",
      "9    spam\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert ham and spam to 0 and 1 respectively\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(dataset[0])\n",
    "\n",
    "print(Y[:10])\n",
    "print(dataset[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "5    FreeMsg Hey there darling it's been 3 week's n...\n",
      "6    Even my brother is not like to speak with me. ...\n",
      "7    As per your request 'Melle Melle (Oru Minnamin...\n",
      "8    WINNER!! As a valued network customer you have...\n",
      "9    Had your mobile 11 months or more? U R entitle...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Getting text messages\n",
    "text_messages = dataset[1]\n",
    "print(text_mes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Pre-processing using Regex\n",
    "Regex from https://www.regexlib.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email addresses\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddr')\n",
    "\n",
    "# Web sites\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', 'unirloc')\n",
    "\n",
    "# Money symbols\n",
    "processed = processed.str.replace(r'£|\\$', 'monysymb')\n",
    "\n",
    "# Phone numbers\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phnnumb')\n",
    "\n",
    "# Numbers\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'nmbr')\n",
    "\n",
    "# remove Punctuations\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# trim white spaces\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To lower case\n",
    "processed = processed.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed = processed.apply(lambda x: ' '.join(w for w in x.split() if w not in stop_words))\n",
    "\n",
    "# Stemming words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "processed = processed.apply(lambda x: ' '.join(stemmer.stem(w) for w in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazi avail bugi n great world...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri nmbr wkli comp win fa cup final tkt...\n",
      "3                     u dun say earli hor u c alreadi say\n",
      "4                    nah think goe usf live around though\n",
      "                              ...                        \n",
      "5567    nmbrnd time tri nmbr contact u u monysymbnmbr ...\n",
      "5568                              ü b go esplanad fr home\n",
      "5569                                    piti mood suggest\n",
      "5570    guy bitch act like interest buy someth els nex...\n",
      "5571                                       rofl true name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Creating a bag of words\n",
    "word_tokens = [w for s in processed for w in word_tokenize(s)]\n",
    "\n",
    "word_tokens = nltk.FreqDist(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 6584 samples and 53376 outcomes>\n",
      "Most common: [('nmbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 451), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('monysymbnmbr', 303), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266), ('like', 261), ('got', 252), ('time', 252), ('good', 248), ('want', 247)]\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)\n",
    "print('Most common: {}'.format(word_tokens.most_common(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 1500 most common words as features\n",
    "word_features = list(word_tokens.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find features in a message\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        features[w] = w in word_features\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go jurong point crazi avail bugi n great world la e buffet cine got amor wat \n",
      " ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
      "ok lar joke wif u oni \n",
      " ['ok', 'lar', 'joke', 'wif', 'u', 'oni']\n",
      "free entri nmbr wkli comp win fa cup final tkt nmbrst may nmbr text fa nmbr receiv entri question std txt rate c appli nmbrovernmbr \n",
      " ['free', 'entri', 'nmbr', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', 'nmbrst', 'may', 'text', 'receiv', 'question', 'std', 'txt', 'rate', 'c', 'appli', 'nmbrovernmbr']\n",
      "u dun say earli hor u c alreadi say \n",
      " ['u', 'dun', 'say', 'earli', 'hor', 'c', 'alreadi']\n",
      "nah think goe usf live around though \n",
      " ['nah', 'think', 'goe', 'usf', 'live', 'around', 'though']\n"
     ]
    }
   ],
   "source": [
    "# Example: Extracting features\n",
    "for m in processed[:5]:\n",
    "    features = find_features(m)\n",
    "    tokens = [key for key, val in features.items() if val is True]\n",
    "    print(m, '\\n', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "messages = list(zip(processed, Y))\n",
    "\n",
    "# seed for reproducibility\n",
    "np.random.seed = 1\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# Finding features and making dataset\n",
    "feature_set = [(find_features(msg), label) for (msg, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(feature_set, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4179\n",
      "Test size: 1393\n"
     ]
    }
   ],
   "source": [
    "print('Train size: {}'.format(len(train)))\n",
    "print('Test size: {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Sci-kit Learn models with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to train\n",
    "names = ['Random Forest', 'Logistic Regression', 'SGD', 'Decision Trees', 'K Neighbors', 'Multinomial NB', 'SVC Linear']\n",
    "classifiers = [RandomForestClassifier(),\n",
    "              LogisticRegression(),\n",
    "              SGDClassifier(max_iter = 100),\n",
    "              DecisionTreeClassifier(),\n",
    "              KNeighborsClassifier(),\n",
    "              MultinomialNB(),\n",
    "              SVC(kernel = 'linear')]\n",
    "\n",
    "models = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model accuracy: 98.49246231155779\n",
      "Logistic Regression model accuracy: 98.7078248384781\n",
      "SGD model accuracy: 98.49246231155779\n",
      "Decision Trees model accuracy: 95.908111988514\n",
      "K Neighbors model accuracy: 94.75951184493898\n",
      "Multinomial NB model accuracy: 98.49246231155779\n",
      "SVC Linear model accuracy: 98.34888729361091\n"
     ]
    }
   ],
   "source": [
    "# Wrap models in nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(train)\n",
    "    acc = accuracy(nltk_model, test) * 100\n",
    "    print('{} model accuracy: {}'.format(name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier model accuracy: 98.7078248384781\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Method: Training all models at once and voting on the result - Voting Classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# voting = 'hard' is taking class labels as votes, 'soft' is taking probabilities as votes(like softmax)\n",
    "# n_jobs = -1 is use all available cores in CPU\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(train)\n",
    "\n",
    "acc = accuracy(nltk_ensemble, test) * 100\n",
    "print('Voting Classifier model accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class label predictions\n",
    "txt_feat, labels = zip(*test)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1209\n",
      "           1       1.00      0.90      0.95       184\n",
      "\n",
      "    accuracy                           0.99      1393\n",
      "   macro avg       0.99      0.95      0.97      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>18</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1209    0\n",
       "       spam        18  166"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating confusion matrix and classification reports\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(labels, prediction),\n",
    "             index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "            columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
